# CSE575-Fairness-in-ML

P17: Fairness in Machine Learning

### Problem: 
Automated decision-making process is increasing the risk of discrimination against certain 
groups of people (e.g. race, gender). Researchers try to find methods to detect algorithmic 
transparency and to debias data and model. Can you develop your own method to (1) detect unfairness 
and (2) make fair decisions without discrimination?

### Data: 
adult income dataset (https://archive.ics.uci.edu/ml/machine-learning-databases/adult/)
German credit data (https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
criminal history dataset (https://github.com/propublica/compas-analysis/) 

### Introductory resources
•Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh 
Venkatasubramanian. 2015. Certifying and Removing Disparate Impact. KDD 2015.
•Datta A., Sen S., Zick Y. (2017) Algorithmic Transparency via Quantitative Input Influence. In: 
Cerquitelli T., Quercia D., Pasquale F. (eds) Transparent Data Mining for Big and Small Data. Studies 
in Big Data, vol 32. Springer, Cham
•Fairness in Machine Learning [https://fairmlclass.github.io/]
•Comments: High impact research. Very hot topics in recent years. Well likely to lead to publication.
